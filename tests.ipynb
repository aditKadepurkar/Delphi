{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (0.24.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from datasets) (3.15.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, yarl, pandas, multiprocess, aiosignal, aiohttp, accelerate, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed accelerate-0.33.0 aiohttp-3.9.5 aiosignal-1.3.1 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-17.0.0 pyarrow-hotfix-0.6 pytz-2024.1 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import Datasets\n",
    "import chromadb\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer, models, losses, util, InputExample, evaluation, SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic functions the model can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingList(model, sentences):\n",
    "  \"\"\" This function returns the sentence embeddings for a given document using the SentenceTransformer model and encapsulates them inside a list.\n",
    "\n",
    "  @param model: SentenceTransformer: The model to be used for getting the embeddings.\n",
    "  @param sentences: list: The list of sentences for which embeddings are to be calculated. \"\"\"\n",
    "\n",
    "  embeddings = model.encode(sentences)\n",
    "  return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel() -> SentenceTransformer:\n",
    "  \"\"\" This function creates a SentenceTransformer model using the 'sentence-transformers/all-MiniLM-L6-v2' base model. It utilizes accelerator to make use of multiple GPUs\n",
    "  and adds a layer to get the sentence embeddings via mean pooling. This model will be used for training sbert's sentence embeddings. \"\"\"\n",
    "\n",
    "  accelerator = Accelerator()\n",
    "  print(f\"Using GPUs: {accelerator.num_processes}\")\n",
    "\n",
    "  # Get the base model to train\n",
    "  word_embedding_model = models.Transformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "  # Add layer to get \"sentence embedding\" (using mean pooling)\n",
    "  pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "  model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An initial list of actions that the AI can choose between\n",
    "SET_OF_ACTIONS = ['new file', 'search web', 'search files', 'resize window', 'choose option', 'open file', 'close file', 'minimize window', 'maximize window', 'scroll up', 'scroll down', 'scroll left', 'scroll right', 'open', 'close', 'upload']\n",
    "# SET_OF_ACTIONS = ['new file', 'search', 'resize window', 'choose option', 'scroll', 'open file', 'close file', 'minimize window', 'maximize window', 'scroll up', 'scroll down', 'scroll left', 'scroll right', 'copy', 'paste', 'cut', 'undo', 'redo', 'drag and drop', 'select', 'deselect', 'save', 'save as', 'open', 'close', 'upload']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "\n",
    "doc_collection = client.get_or_create_collection(\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "model = getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Exercise regularly and stay fit.\\n2. Learn a new programming language.\\n3. Read at least one book every month.\\n4. Spend more time with family and friends.\\n5. Travel to at least two new places.\\n6. Save money and stick to a budget.\\n7. Volunteer for a good cause.\\n8. Improve my communication skills.\\n9. Learn a musical instrument.\\n10. Practice mindfulness and reduce stress.', 'def bubble_sort(arr):\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n-i-1):\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n\\ndef selection_sort(arr):\\n    n = len(arr)\\n    for i in range(n):\\n        min_idx = i\\n        for j in range(i+1, n):\\n            if arr[j] < arr[min_idx]:\\n                min_idx = j\\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\\n\\ndef insertion_sort(arr):\\n    n = len(arr)\\n    for i in range(1, n):\\n        key = arr[i]\\n        j = i-1\\n        while j >= 0 and arr[j] > key:\\n            arr[j+1] = arr[j]\\n            j -= 1\\n        arr[j+1] = key', 'import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Send a GET request to the website you want to scrape\\nurl = \"https://example.com\"\\nresponse = requests.get(url)\\n\\n# Create a BeautifulSoup object to parse the HTML content\\nsoup = BeautifulSoup(response.content, \"html.parser\")\\n\\n# Find and extract the desired data from the HTML\\n# For example, let\\'s extract all the links on the page\\nlinks = soup.find_all(\"a\")\\nfor link in links:\\n    print(link.get(\"href\"))', 'Life is a journey filled with ups and downs, twists and turns. It is a beautiful tapestry of experiences, emotions, and lessons. Each day brings new opportunities and challenges, shaping us into who we are. We strive to find meaning and purpose, to make a positive impact on the world around us. Life is a precious gift, and it is up to us to embrace it fully and live it to the fullest.']\n",
      "['resolutions.txt', 'sorting.py', 'random.py', 'example.txt']\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"test/\"  # Replace with the actual directory path\n",
    "\n",
    "file_list = []\n",
    "for file_name in os.listdir(dir_path):\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "        file_list.append(data)\n",
    "        \n",
    "print(file_list)\n",
    "\n",
    "file_names = [file_name for file_name in os.listdir(dir_path)]\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: id0\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Insert of existing embedding ID: id0\n",
      "Insert of existing embedding ID: id1\n",
      "Insert of existing embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "doc_collection.add(\n",
    "    embeddings=\n",
    "        getEmbeddingList(model, file_list)\n",
    "    ,\n",
    "    documents=file_names,\n",
    "    ids=[f'id{i}' for i in range(len(file_list))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune this or look at online examples because current outputs are bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id0']], 'distances': [[59.03951644897461]], 'metadatas': [[None]], 'embeddings': None, 'documents': [['sorting.py']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# bad\n",
    "input = \"Python codes for web scraping\"\n",
    "\n",
    "query_result = doc_collection.query(\n",
    "            query_embeddings=[getEmbeddingList(model, input)],\n",
    "            n_results=1,\n",
    "        )\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id2']], 'distances': [[46.51013946533203]], 'metadatas': [[None]], 'embeddings': None, 'documents': [['example.txt']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# bad\n",
    "input = \"new years resolutions\"\n",
    "\n",
    "query_result = doc_collection.query(\n",
    "            query_embeddings=[getEmbeddingList(model, input)],\n",
    "            n_results=1,\n",
    "        )\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to get a vocal dataset from hf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Delphi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
