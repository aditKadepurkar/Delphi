{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (0.24.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from datasets) (3.15.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, yarl, pandas, multiprocess, aiosignal, aiohttp, accelerate, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed accelerate-0.33.0 aiohttp-3.9.5 aiosignal-1.3.1 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-17.0.0 pyarrow-hotfix-0.6 pytz-2024.1 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import Datasets\n",
    "import chromadb\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer, models, losses, util, InputExample, evaluation, SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "import glob\n",
    "import os\n",
    "import ollama\n",
    "# from langchain.text_splitter import NLTKTextSplitter\n",
    "# from langchain_community.document_loaders import (\n",
    "#     # PDFLoader,\n",
    "#     # WordDocumentLoader,\n",
    "#     TextLoader,\n",
    "#     # ExcelLoader,\n",
    "#     CSVLoader,\n",
    "#     # PowerPointLoader\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingList(model, sentences):\n",
    "  \"\"\" This function returns the sentence embeddings for a given document using the SentenceTransformer model and encapsulates them inside a list.\n",
    "\n",
    "  @param model: SentenceTransformer: The model to be used for getting the embeddings.\n",
    "  @param sentences: list: The list of sentences for which embeddings are to be calculated. \"\"\"\n",
    "\n",
    "  embeddings = model.encode(sentences)\n",
    "  return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel() -> SentenceTransformer:\n",
    "  \"\"\" This function creates a SentenceTransformer model using the 'sentence-transformers/all-MiniLM-L6-v2' base model. It utilizes accelerator to make use of multiple GPUs\n",
    "  and adds a layer to get the sentence embeddings via mean pooling. This model will be used for training sbert's sentence embeddings. \"\"\"\n",
    "\n",
    "  accelerator = Accelerator()\n",
    "  print(f\"Using GPUs: {accelerator.num_processes}\")\n",
    "\n",
    "  # Get the base model to train\n",
    "  word_embedding_model = models.Transformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "  # Add layer to get \"sentence embedding\" (using mean pooling)\n",
    "  pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "  model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An initial list of actions that the AI can choose between\n",
    "SET_OF_ACTIONS = ['new file', 'search web', 'search files', 'resize window', 'choose option', 'open file', 'close file', 'minimize window', 'maximize window', 'scroll up', 'scroll down', 'scroll left', 'scroll right', 'open', 'close', 'upload']\n",
    "# SET_OF_ACTIONS = ['new file', 'search', 'resize window', 'choose option', 'scroll', 'open file', 'close file', 'minimize window', 'maximize window', 'scroll up', 'scroll down', 'scroll left', 'scroll right', 'copy', 'paste', 'cut', 'undo', 'redo', 'drag and drop', 'select', 'deselect', 'save', 'save as', 'open', 'close', 'upload']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic file gathering and putting into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(initdir: str, file_extensions: list):\n",
    "    '''\n",
    "    Returns a list of file under initdir and all its subdirectories\n",
    "    that have file extension contained in file_extensions.\n",
    "    ''' \n",
    "    file_list = []\n",
    "    file_count = {key: 0 for key in file_extensions}  # for reporting only\n",
    "    \n",
    "    # Traverse through directories to find files with specified extensions\n",
    "    for root, _, files in os.walk(initdir):\n",
    "        for file in files:\n",
    "            ext = file.split('.')[-1].lower()\n",
    "            if ext in file_extensions:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_list.append(file_path)\n",
    "                # increment type of file\n",
    "                file_count[ext] += 1\n",
    "    \n",
    "    # total = len(file_list)\n",
    "    # print(f'There are {total} files under dir {initdir}.')\n",
    "    # for k, n in file_count.items():\n",
    "        # print(f'   {n} : \".{k}\" files')\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/resolutions.txt',\n",
       " 'test/sorting.py',\n",
       " 'test/random.py',\n",
       " 'test/example.txt',\n",
       " 'test/buhao.c']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files('test', ['txt', 'c', 'py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_info(file_path: str):\n",
    "    '''\n",
    "    Open the file at the given file path and return its content.\n",
    "    \n",
    "    @param file_path: str: The path of the file to be opened.\n",
    "    @return: str: The content of the file.\n",
    "    '''\n",
    "    try: \n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        # metadata = file.metadata\n",
    "        file_name = os.path.basename(file_path)\n",
    "        return (file_name, content)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('resolutions.txt',\n",
       " '1. Exercise regularly and stay fit.\\n2. Learn a new programming language.\\n3. Read at least one book every month.\\n4. Spend more time with family and friends.\\n5. Travel to at least two new places.\\n6. Save money and stick to a budget.\\n7. Volunteer for a good cause.\\n8. Improve my communication skills.\\n9. Learn a musical instrument.\\n10. Practice mindfulness and reduce stress.')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_document_info('test/resolutions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama implementation for better semantics(you need an ollama server running in the background for this to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = '''\n",
    "FROM llama3\n",
    "SYSTEM You are supposed to give a description of the input and nothing else.\n",
    "'''\n",
    "\n",
    "ollama.create(model='example', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'example',\n",
       " 'created_at': '2024-07-28T19:39:09.125459Z',\n",
       " 'message': {'role': 'assistant', 'content': \"New Year's resolutions!\"},\n",
       " 'done': True,\n",
       " 'total_duration': 1382438791,\n",
       " 'load_duration': 1032208,\n",
       " 'prompt_eval_count': 124,\n",
       " 'prompt_eval_duration': 1096374000,\n",
       " 'eval_count': 6,\n",
       " 'eval_duration': 276858000}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.chat(model=\"example\", messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f'{get_document_info('test/resolutions.txt')[1]}'\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_description(file_path: str, modelfile: str):\n",
    "    '''\n",
    "    Get the description of the input from the Ollama model.\n",
    "    \n",
    "    @param file_path: str: The file with the document.\n",
    "    @param modelfile: str: The modelfile for the Ollama model.\n",
    "    @return: str: The description of the input.\n",
    "    '''\n",
    "    content = get_document_info(file_path)\n",
    "    ollama.create(model='example', modelfile=modelfile)\n",
    "    response = ollama.chat(model=\"example\", messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Filename: {content[0]}, File content:{content[1]}'\n",
    "        }])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"New Year's resolutions text file containing 10 goals for personal improvement, including exercise, learning, travel, financial management, social connections, and self-care.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ollama_description('test/resolutions.txt', modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedmodel = getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "\n",
    "doc_collection = client.get_or_create_collection(\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection.add(\n",
    "    embeddings=\n",
    "        getEmbeddingList(embedmodel, file_list)\n",
    "    ,\n",
    "    documents=file_names,\n",
    "    ids=[f'id{i}' for i in range(len(file_list))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune this or look at online examples because current outputs are bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id0']], 'distances': [[59.03951644897461]], 'metadatas': [[None]], 'embeddings': None, 'documents': [['sorting.py']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# bad\n",
    "input = \"Python codes for web scraping\"\n",
    "\n",
    "query_result = doc_collection.query(\n",
    "            query_embeddings=[getEmbeddingList(model, input)],\n",
    "            n_results=1,\n",
    "        )\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id2']], 'distances': [[46.51013946533203]], 'metadatas': [[None]], 'embeddings': None, 'documents': [['example.txt']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# bad\n",
    "input = \"new years resolutions\"\n",
    "\n",
    "query_result = doc_collection.query(\n",
    "            query_embeddings=[getEmbeddingList(model, input)],\n",
    "            n_results=1,\n",
    "        )\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to get a vocal dataset from hf to do tests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Delphi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
