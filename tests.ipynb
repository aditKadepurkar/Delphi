{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/Delphi/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import Datasets\n",
    "import chromadb\n",
    "from chromadb import Collection\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer, models, losses, util, InputExample, evaluation, SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "import glob\n",
    "import os\n",
    "import ollama\n",
    "# from langchain.text_splitter import NLTKTextSplitter\n",
    "# from langchain_community.document_loaders import (\n",
    "#     # PDFLoader,\n",
    "#     # WordDocumentLoader,\n",
    "#     TextLoader,\n",
    "#     # ExcelLoader,\n",
    "#     CSVLoader,\n",
    "#     # PowerPointLoader\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingList(model, sentences):\n",
    "  \"\"\" This function returns the sentence embeddings for a given document using the SentenceTransformer model and encapsulates them inside a list.\n",
    "\n",
    "  @param model: SentenceTransformer: The model to be used for getting the embeddings.\n",
    "  @param sentences: list: The list of sentences for which embeddings are to be calculated. \"\"\"\n",
    "\n",
    "  embeddings = model.encode(sentences)\n",
    "  return embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel() -> SentenceTransformer:\n",
    "  \"\"\" This function creates a SentenceTransformer model using the 'sentence-transformers/all-MiniLM-L6-v2' base model. It utilizes accelerator to make use of multiple GPUs\n",
    "  and adds a layer to get the sentence embeddings via mean pooling. This model will be used for training sbert's sentence embeddings. \"\"\"\n",
    "\n",
    "  accelerator = Accelerator()\n",
    "  print(f\"Using GPUs: {accelerator.num_processes}\")\n",
    "\n",
    "  # Get the base model to train\n",
    "  word_embedding_model = models.Transformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "  # Add layer to get \"sentence embedding\" (using mean pooling)\n",
    "  pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "  model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An initial list of actions that the AI can choose between\n",
    "SET_OF_ACTIONS = ['new file', 'search web', 'search files', 'resize window', 'choose option', 'open file', 'close file', 'minimize window', 'maximize window', 'scroll up', 'scroll down', 'scroll left', 'scroll right', 'open', 'close', 'upload']\n",
    "# SET_OF_ACTIONS = ['new file', 'search', 'resize window', 'choose option', 'scroll', 'open file', 'close file', 'minimize window', 'maximize window', 'scroll up', 'scroll down', 'scroll left', 'scroll right', 'copy', 'paste', 'cut', 'undo', 'redo', 'drag and drop', 'select', 'deselect', 'save', 'save as', 'open', 'close', 'upload']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic file gathering and putting into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(initdir: str, file_extensions: list):\n",
    "    '''\n",
    "    Returns a list of file under initdir and all its subdirectories\n",
    "    that have file extension contained in file_extensions.\n",
    "    ''' \n",
    "    file_list = []\n",
    "    file_count = {key: 0 for key in file_extensions}  # for reporting only\n",
    "    \n",
    "    # Traverse through directories to find files with specified extensions\n",
    "    for root, _, files in os.walk(initdir):\n",
    "        for file in files:\n",
    "            ext = file.split('.')[-1].lower()\n",
    "            if ext in file_extensions:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_list.append(file_path)\n",
    "                # increment type of file\n",
    "                file_count[ext] += 1\n",
    "    \n",
    "    # total = len(file_list)\n",
    "    # print(f'There are {total} files under dir {initdir}.')\n",
    "    # for k, n in file_count.items():\n",
    "        # print(f'   {n} : \".{k}\" files')\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/resolutions.txt',\n",
       " 'test/sorting.py',\n",
       " 'test/random.py',\n",
       " 'test/example.txt',\n",
       " 'test/buhao.c']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files('test', ['txt', 'c', 'py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_info(file_path: str):\n",
    "    '''\n",
    "    Open the file at the given file path and return its content.\n",
    "    \n",
    "    @param file_path: str: The path of the file to be opened.\n",
    "    @return: str: The content of the file.\n",
    "    '''\n",
    "    try: \n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        # metadata = file.metadata\n",
    "        file_name = file_path\n",
    "        return (file_name, content)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test/resolutions.txt',\n",
       " '1. Exercise regularly and stay fit.\\n2. Learn a new programming language.\\n3. Read at least one book every month.\\n4. Spend more time with family and friends.\\n5. Travel to at least two new places.\\n6. Save money and stick to a budget.\\n7. Volunteer for a good cause.\\n8. Improve my communication skills.\\n9. Learn a musical instrument.\\n10. Practice mindfulness and reduce stress.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_document_info('test/resolutions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama implementation for better semantics(you need an ollama server running in the background for this to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfile = '''\n",
    "FROM llama3\n",
    "SYSTEM You are have to give the file type, a description of the input, and NOTHING else.\n",
    "'''\n",
    "\n",
    "ollama.create(model='example', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'example',\n",
       " 'created_at': '2024-07-29T01:10:25.099018Z',\n",
       " 'message': {'role': 'assistant', 'content': \"New Year's resolutions!\"},\n",
       " 'done': True,\n",
       " 'total_duration': 9681068833,\n",
       " 'load_duration': 8441893166,\n",
       " 'prompt_eval_count': 125,\n",
       " 'prompt_eval_duration': 955635000,\n",
       " 'eval_count': 6,\n",
       " 'eval_duration': 278605000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.chat(model=\"example\", messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f'{get_document_info('test/resolutions.txt')[1]}'\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_description(file_path: str, modelfile: str):\n",
    "    \"\"\"\n",
    "    Get the description of the input from the Ollama model.\n",
    "    \n",
    "    @param file_path: str: The file with the document.\n",
    "    @param modelfile: str: The modelfile for the Ollama model.\n",
    "    @return: str: The description of the input.\n",
    "    \"\"\"\n",
    "    content = get_document_info(file_path)\n",
    "    ollama.create(model='example', modelfile=modelfile)\n",
    "    response = ollama.chat(model=\"example\", messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Filename: {os.path.basename(content[0])}, File content:{content[1]}'\n",
    "        }])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ollama_description('test/buhao.c', modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "embedmodel = getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "\n",
    "doc_collection = client.get_or_create_collection(\"docs1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_database(file_list: list, collection: Collection, embedmodel: SentenceTransformer, modelfile: str):\n",
    "    # sentences = []\n",
    "    # for file in file_list:\n",
    "    #     get_ollama_description(file_path=file, modelfile=modelfile)\n",
    "    \n",
    "    sentences = []\n",
    "    for file in file_list:\n",
    "        sentences.append(get_ollama_description(file_path=file, modelfile=modelfile))\n",
    "    embeds = getEmbeddingList(model=embedmodel, sentences=sentences)\n",
    "    collection.add(\n",
    "        embeddings=embeds,\n",
    "        documents=file_list,\n",
    "        ids=[f'id{i}' for i in range(len(file_list))],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_database(file_list=list_files('test', ['txt', 'c', 'py']), collection=doc_collection, embedmodel=embedmodel, modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune this or look at online examples because current outputs are bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id2', 'id1', 'id3', 'id4', 'id0']], 'distances': [[10.741991996765137, 56.16775894165039, 77.28575134277344, 83.09525299072266, 85.8222885131836]], 'metadatas': [[None, None, None, None, None]], 'embeddings': None, 'documents': [['test/random.py', 'test/sorting.py', 'test/example.txt', 'test/buhao.c', 'test/resolutions.txt']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# bad\n",
    "input = \"Python codes for web scraping\"\n",
    "\n",
    "query_result = doc_collection.query(\n",
    "            query_embeddings=[getEmbeddingList(embedmodel, input)],\n",
    "            n_results=5,\n",
    "        )\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id0', 'id1', 'id3', 'id4', 'id2']], 'distances': [[25.02292251586914, 59.867645263671875, 63.078582763671875, 85.54891967773438, 104.3021240234375]], 'metadatas': [[None, None, None, None, None]], 'embeddings': None, 'documents': [['test/resolutions.txt', 'test/sorting.py', 'test/example.txt', 'test/buhao.c', 'test/random.py']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# bad\n",
    "input = \"new years resolutions\"\n",
    "\n",
    "query_result = doc_collection.query(\n",
    "            query_embeddings=[getEmbeddingList(embedmodel, input)],\n",
    "            n_results=5,\n",
    "        )\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_file(file_path: str, content: str):\n",
    "    '''\n",
    "    Create a file at the given file path with the given content.\n",
    "    \n",
    "    @param file_path: str: The path of the file to be created.\n",
    "    @param content: str: The content of the file.\n",
    "    '''\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(file_path: str):\n",
    "    '''\n",
    "    Remove the file at the given file path.\n",
    "    \n",
    "    @param file_path: str: The path of the file to be removed.\n",
    "    '''\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_files(query: str, collection: Collection, embedmodel: SentenceTransformer):\n",
    "    '''\n",
    "    Search for files in the collection that match the query.\n",
    "    \n",
    "    @param query: str: The query to search for.\n",
    "    @param collection: Collection: The collection to search in.\n",
    "    @param embedmodel: SentenceTransformer: The model to be used for getting the embeddings.\n",
    "    @return: list: The list of files that match the query.\n",
    "    '''\n",
    "    query_result = collection.query(\n",
    "        query_embeddings=[getEmbeddingList(embedmodel, query)],\n",
    "        n_results=5,\n",
    "    )\n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_path: str):\n",
    "    '''\n",
    "    Open the file at the given file path.\n",
    "    \n",
    "    @param file_path: str: The path of the file to be opened.\n",
    "    '''\n",
    "    os.system(f'xdg-open {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to get a vocal dataset from hf to do tests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Delphi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
